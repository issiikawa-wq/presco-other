import os
import time
import csv
import json
import re
from datetime import datetime
from zoneinfo import ZoneInfo

from playwright.sync_api import sync_playwright
import gspread
from oauth2client.service_account import ServiceAccountCredentials


# =====================================
# Presco ãƒ­ã‚°ã‚¤ãƒ³ï¼†CSVå–å¾—ï¼ˆ1é€±é–“ï¼‰
# =====================================
def login_and_download_csv():

    email = os.getenv("PRESCO_EMAIL")
    password = os.getenv("PRESCO_PASSWORD")

    with sync_playwright() as p:

        browser = p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-setuid-sandbox",
                "--disable-blink-features=AutomationControlled"
            ]
        )

        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/120.0.0.0 Safari/537.36"
        )

        page = context.new_page()

        try:
            # ãƒ­ã‚°ã‚¤ãƒ³
            page.goto("https://presco.ai/partner/", timeout=60000)
            page.wait_for_selector('input[name="username"]')

            page.fill('input[name="username"]', email)
            page.fill('input[name="password"]', password)

            with page.expect_navigation():
                page.click('input[type="submit"]')

            print("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")

            # æˆæœä¸€è¦§ãƒšãƒ¼ã‚¸
            page.goto("https://presco.ai/partner/actionLog/list")
            page.wait_for_load_state("networkidle")
            time.sleep(3)

            # æˆæœç™ºç”Ÿæ—¥æ™‚ã«å¤‰æ›´
            selectors = [
                'input[name="dateType"][value="actionDate"]',
                'input[type="radio"][value="actionDate"]',
                'label:has-text("æˆæœç™ºç”Ÿæ—¥æ™‚")'
            ]

            for selector in selectors:
                try:
                    page.click(selector, timeout=5000)
                    break
                except:
                    continue

            time.sleep(1)

            # ğŸ”¥ 1é€±é–“ï¼ˆonclickç›´æ¥æŒ‡å®šï¼‰
            page.click('a[onclick="setWeek()"]', timeout=10000)
            time.sleep(1)

            # æ¤œç´¢ãƒœã‚¿ãƒ³ï¼ˆdivï¼‰
            page.click('.filter-button--submit', timeout=10000)
            time.sleep(5)

            page.wait_for_selector("#csv-link")

            with page.expect_download() as download_info:
                page.click("#csv-link")

            download = download_info.value
            csv_path = "/tmp/presco_week.csv"
            download.save_as(csv_path)

            print("CSVå–å¾—å®Œäº†")

            return csv_path

        finally:
            browser.close()


# =====================================
# 2026/02/20 00:00:00 ä»¥é™ãƒ•ã‚£ãƒ«ã‚¿
# =====================================
def get_cutoff_datetime():
    JST = ZoneInfo("Asia/Tokyo")
    return datetime(2026, 2, 20, 0, 0, 0, tzinfo=JST)


def is_after_cutoff(date_string, cutoff):
    try:
        JST = ZoneInfo("Asia/Tokyo")
        dt = datetime.strptime(date_string, "%Y/%m/%d %H:%M:%S")
        dt = dt.replace(tzinfo=JST)
        return dt >= cutoff
    except:
        return False


# =====================================
# CSVå¤‰æ›
# =====================================
def extract_gclid(url):
    if not url:
        return ""
    match = re.search(r"gclid=([^&]+)", url)
    return match.group(1) if match else ""


def transform_csv(csv_path):

    target_sites = ["Fast Baito ä»‹è­·ç‰¹åŒ–", "Fast Baito"]
    cutoff = get_cutoff_datetime()

    print("ã‚«ãƒƒãƒˆã‚ªãƒ•æ—¥æ™‚:", cutoff)

    with open(csv_path, "r", encoding="shift_jis", errors="ignore") as f:
        reader = list(csv.reader(f))

    data = reader[1:]

    results = []
    results.append(["Parameters:TimeZone=Asia/Tokyo"])
    results.append([
        "Google Click ID",
        "Conversion Name",
        "Conversion Time",
        "Conversion Value",
        "Conversion Currency"
    ])

    seen = set()

    for row in data:

        if len(row) < 18:
            continue

        site = row[5]
        if site not in target_sites:
            continue

        action_datetime = row[3]

        if not is_after_cutoff(action_datetime, cutoff):
            continue

        gclid = extract_gclid(row[12])
        if not gclid:
            continue

        if gclid in seen:
            continue

        seen.add(gclid)

        if site == "Fast Baito ä»‹è­·ç‰¹åŒ–":
            value = "3000"
            conv_name = "ä»‹è­·ã‚ªãƒ•ãƒ©ã‚¤ãƒ³CV"
        else:
            value = str(int(float(row[17])))
            conv_name = "ã‚ªãƒ•ãƒ©ã‚¤ãƒ³CV"

        results.append([
            gclid,
            conv_name,
            action_datetime,
            value,
            "JPY"
        ])

    print("æŠ½å‡ºä»¶æ•°:", len(results) - 2)

    return results


# =====================================
# Sheetsæ›¸ãè¾¼ã¿
# =====================================
def upload_to_sheet(data):

    creds_json = os.getenv("GOOGLE_CREDENTIALS")
    sheet_id = os.getenv("SPREADSHEET_ID")

    creds = json.loads(creds_json)

    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/drive"
    ]

    credentials = ServiceAccountCredentials.from_json_keyfile_dict(creds, scope)
    gc = gspread.authorize(credentials)

    sh = gc.open_by_key(sheet_id)
    ws = sh.worksheet("æˆæœæƒ…å ±_ãã®ä»–")

    ws.clear()
    ws.update(values=data, range_name="A1")

    print("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†")


# =====================================
# main
# =====================================
def main():

    csv_path = login_and_download_csv()
    transformed = transform_csv(csv_path)
    upload_to_sheet(transformed)

    print("å®Œäº†")


if __name__ == "__main__":
    main()
